<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://www.cpdis.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.cpdis.com/" rel="alternate" type="text/html" /><updated>2018-01-17T13:47:18-06:00</updated><id>https://www.cpdis.com/</id><title type="html">Colin Dismuke</title><subtitle>A blog about AI, cryptoassets, and running.</subtitle><entry><title type="html">PathNet - Evolution Channels Gradient Descent in Super Neural Networks</title><link href="https://www.cpdis.com/PathNet-A-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/" rel="alternate" type="text/html" title="PathNet - Evolution Channels Gradient Descent in Super Neural Networks" /><published>2018-01-17T13:40:10-06:00</published><updated>2018-01-17T13:40:10-06:00</updated><id>https://www.cpdis.com/PathNet-A-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks</id><content type="html" xml:base="https://www.cpdis.com/PathNet-A-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/">&lt;p&gt;&lt;em&gt;Note: Inevitably, this past week was much more busy than planned. As such, I didn‚Äôt get to explore much more than the paper itself and the Github repository. I‚Äôd rather not do the minimum amount of work to achieve a goal but in this case it‚Äôs the best I can do.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I first came across PathNet in Azeem Azhar‚Äôs essential &lt;a href=&quot;https://www.getrevue.co/profile/azeem?utm_campaign=Issue&amp;amp;utm_content=forwarded&amp;amp;utm_medium=email&amp;amp;utm_source=Azeem+Azhar%3A+The+Exponential+View&quot;&gt;The Exponential View&lt;/a&gt; newsletter almost exactly a year ago. DeepMind was causing a stir in the AI community because PathNet was a plausible precursor to an architecture that could support artificial general intelligence (AGI). PathNet combines modular deep learning, meta-learning, and reinforcement learning and is summarized this way in the introduction to the &lt;a href=&quot;https://arxiv.org/abs/1701.08734&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Neural networks, in general, are trained on data for each specific task they are trying to achieve. This is time consuming and not efficient. Transfer learning was developed to bypass this problem but has limited use. PathNet goes beyond transfer learning, where knowledge gained while solving one problem is applied to a different but related problem, it finds the best parameters to be reused for transfer learning and implements those. Essentially, a neural network of neural networks.&lt;/p&gt;

&lt;p&gt;A PathNet is a modular deep neural network with any number of layers consisting of modules. Each module within each layer is itself a neural network (see last sentence of the previous paragraph). Each neural network module is either convolutional or linear and is followed by a transfer function (RELUs in this case). At each layer the output of each module is summed and then passed on to the next layer. While there may be an arbitrary number of modules per layer, typically a maximum of 3 or 4 distinct modules are permitted in the final pathway. The final layer in a PathNet is unique and not shared between different tasks. The figure belows shows this model in action. The first three layers are convolutional 2D kernels with 8 kernels per module (the green boxes in the figure), kernel sizes of (8, 4, 3), and strides (4, 2, 1) from the first to the third layer, respectively. After each module is a RELU and the layers are summed before being passed on to the next layer (light blue boxes). The red boxes show the modules that are passed on to the next layer, if all modules were included then as the model evolved it would simply grow to encompass the entire network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/atari.jpg&quot; alt=&quot;Atari&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The tasks that were considered were MNIST classification, CIFAR and SVHN, several Atari games, and several Labyrinth games. For binary MNIST classification the researchers found that PathNet helped speed up learning in the classification task by decreasing the mean time to solution from 229 generations to 167 generations. They found this to be the case for both the control (independent learning) and when the hyperparameters were fine tuned. The speedup ratio compared to independent learning was 1.18. The histograms below clearly show the reduction in the number of generations to achieve 0.998 accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/MNIST.jpg&quot; alt=&quot;MNIST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moving on to the Atari games, the researchers found that PathNet was superior to fine-tuning. fine-tuning was performed by doing a hyperparameter sweep of learning rates and entropy costs while PathNet was investigated using a range of evaluation times, mutation rates, and tournament sizes. [I understand that it‚Äôs necessary to tune the model to achieve optimal results, however, if you must tune PathNet doesn‚Äôt that make it a little less viable as AGI?] An optimal combination of tournament size and mutation rate were found for PathNet that achieved rapid convergence and a speedup ratio of 1.33 versus 1.16 for fine-tuning. The figure below shows the results for the first 40 million steps of training for PathNet (blue), fine-tuning (green), and independent learning (red). The results for both PathNet and fine-tuning show the top five hyperparameter settings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Atari2/atari2.jpg&quot; alt=&quot;Atari2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, three labyrinth games were tested, &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;stairway_to_melon&lt;/code&gt;. All of the games are part of DeepMind‚Äôs &lt;a href=&quot;https://github.com/deepmind/lab&quot;&gt;DeepMind Lab&lt;/a&gt;. Again, a hyperparameter sweep was used for fine-tuning; mutation rates, module duplication rates, and tournament size were varied while learning rate, entropy cost, and evaluation time were fixed. PathNet learns the second task faster than fine tuning for transfer to &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt; and transfer from &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt;. PathNet also performs better when learning &lt;code class=&quot;highlighter-rouge&quot;&gt;stairway_to_melon&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt; from scratch. Interestingly, when transferring to &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt;, both fine tuning and PathNet perform worse than independent learning. Speedup for PathNet is 1.26 versus 1.0 for fine-tuning (this is skewed by the good performance of transferring from &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;stairway_to_melon&lt;/code&gt;). The figure below shows the mean of the five best training runes for PathNet compared with fine-tuning (the off diagonal plots) and independent learning (diagonal plots labeled from scratch). The results are more mixed than the previous examples, however, in most cases PathNet performs better than the control. especially when transferring from one game to another.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/labyrinth.jpg&quot; alt=&quot;Labyrinth&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It‚Äôs pretty clear that PathNet represents a step toward AGI. I wish that I had more time to look at the &lt;a href=&quot;https://github.com/jaesik817/pathnet&quot;&gt;code&lt;/a&gt;, play with it, and see it in action with some of the examples from the paper but I‚Äôm unfortunately already behind with this project.&lt;/p&gt;

&lt;p&gt;The code, notes, and reference files for this week are in this &lt;a href=&quot;https://github.com/cpdis/Experiments/tree/master/1_DeepMind_PathNet&quot;&gt;repository&lt;/a&gt;.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="transfer learning" /><category term="evolution learning" /><category term="continual learning" /><category term="giant networks" /><category term="yearofml" /><summary type="html">Note: Inevitably, this past week was much more busy than planned. As such, I didn‚Äôt get to explore much more than the paper itself and the Github repository. I‚Äôd rather not do the minimum amount of work to achieve a goal but in this case it‚Äôs the best I can do.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">Folder Structure and Workflow</title><link href="https://www.cpdis.com/Folder-Structure-and-Workflow/" rel="alternate" type="text/html" title="Folder Structure and Workflow" /><published>2018-01-08T22:00:18-06:00</published><updated>2018-01-08T22:00:18-06:00</updated><id>https://www.cpdis.com/Folder-Structure-and-Workflow</id><content type="html" xml:base="https://www.cpdis.com/Folder-Structure-and-Workflow/">&lt;p&gt;There are two categories of projects that I‚Äôve collected: ML papers and tutorials/working examples. For both of these I want a consistent workflow that allows me to present summaries in an quick and efficient manner and to reference in the future.&lt;/p&gt;

&lt;h2 id=&quot;papers&quot;&gt;Papers&lt;/h2&gt;
&lt;p&gt;I think that I have come up with a decent template for presenting a summary of each of the papers that I read as well as describe how they might be useful. There will be four sections: &lt;em&gt;Summary&lt;/em&gt;, &lt;em&gt;Notes&lt;/em&gt;, &lt;em&gt;Research Method&lt;/em&gt;, and &lt;em&gt;Resources&lt;/em&gt;. The &lt;em&gt;Summary&lt;/em&gt; section will include a brief summary in my own words along with a concise summary quote from the paper itself. The &lt;em&gt;Notes&lt;/em&gt; section is pretty self explanatory but I will try to make it verbose enough that it can stand on its own. The &lt;em&gt;Research Method&lt;/em&gt; section will follow this format:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Read the introduction and summarize.&lt;/li&gt;
  &lt;li&gt;Identify the big question or hypothesis.&lt;/li&gt;
  &lt;li&gt;Summarize the background in five sentences or less.&lt;/li&gt;
  &lt;li&gt;Identify specific questions.&lt;/li&gt;
  &lt;li&gt;Identify the approach.&lt;/li&gt;
  &lt;li&gt;Read the Methods section and diagram the experiment (this will vary widely based on the paper).&lt;/li&gt;
  &lt;li&gt;Summarize the findings of each result.&lt;/li&gt;
  &lt;li&gt;Do the results answer the specific questions asked above?&lt;/li&gt;
  &lt;li&gt;Read the conclusion and summarize.&lt;/li&gt;
  &lt;li&gt;What are others saying about this paper?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the &lt;em&gt;Resources&lt;/em&gt; section will link to any additional information about the paper such as code repositories, datasets, subsequent papers, and projects based on the results of the paper.&lt;/p&gt;

&lt;h2 id=&quot;tutorials-and-other-code-based-projects&quot;&gt;Tutorials and other code based projects&lt;/h2&gt;
&lt;p&gt;My goal when working through tutorials or trying to reproduce models is to have a consistent and efficient workflow that makes it simple to replicate across projects. An efficient workflow makes it easier to understand the scope of the project and return to it at a later. At work, despite our best intentions and templated folder structure, our projects inevitably end up as a labyrinth of cryptically named folders full of unlabeled data and results. I hope that starting this project with a carefully considered organizational philosophy will help in the weeks and months to come. A few requirements that went into building my final workflow:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Always use version control.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is important because it makes it easier to work from multiple computers (and iPads), makes it easier to share and collaborate with others, and makes it easier to replicate results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Separate code from data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is especially important in machine learning projects since datasets can be very, very large. In addition, it makes it easier to swap between datasets and share code with others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Separate raw, working, and processed data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I think it‚Äôs useful to separate data into a few different sources:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Raw&lt;/strong&gt; data is the original, immutable data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interim&lt;/strong&gt; data is the working data that is being transformed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Processed&lt;/strong&gt; data is the final dataset being used for modeling.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;External&lt;/strong&gt; data is from third party sources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Organizing the data this way allows you to know when you can safely delete and move files.&lt;/p&gt;

&lt;p&gt;Given those requirements I went about building my folder structure and workflow. Quickly, though, it dawned on me that there are thousands of teams and tens of thousands of practitioners working on real, in-production problems that have most likely optimized their workflows for maximum efficiency. With that, I went in search of the perfect folder structure and research workflow. While I‚Äôm pretty sure I didn‚Äôt find exactly that, I found something that fits all the requirements above and is automated as well.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://drivendata.github.io/cookiecutter-data-science/&quot;&gt;Cookiecutter Data Science&lt;/a&gt;  project structure is &lt;em&gt;a logical, reasonably standardized, but flexible project structure for doing and sharing data science work.&lt;/em&gt; Cookiecutter Data Science is built on &lt;a href=&quot;http://cookiecutter.readthedocs.io/en/latest/readme.html&quot;&gt;Cookiecutter&lt;/a&gt; which is a command-line utility that creates projects from templates (cookiecutters). The creators of Cookiecutter Data Science summarize it like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;When we think about data analysis, we often think just about the resulting reports, insights, or visualizations. While these end products are generally the main event, it‚Äôs easy to focus on making the products &lt;em&gt;look nice&lt;/em&gt; and ignore the &lt;em&gt;quality of the code that generates them&lt;/em&gt;. Because these end products are created programmatically, code quality is still important! And we‚Äôre not talking about bikeshedding the indentation aesthetics or pedantic formatting standards ‚Äî ultimately, data science code quality is about correctness and reproducibility.&lt;/p&gt;

  &lt;p&gt;It‚Äôs no secret that good analyses are often the result of very scattershot and serendipitous explorations. Tentative experiments and rapidly testing approaches that might not work out are all part of the process for getting to the good stuff, and there is no magic bullet to turn data exploration into a simple, linear progression.&lt;/p&gt;

  &lt;p&gt;That being said, once started it is not a process that lends itself to thinking carefully about the structure of your code or project layout, so it‚Äôs best to start with a clean, logical structure and stick to it throughout. We think it‚Äôs a pretty big win all around to use a fairly standardized setup like this one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think that the template based approach is great. I have already modified the directory structure and some make files that I don‚Äôt see myself using initially. As the weeks pass and I refine my workflow, I‚Äôm sure that I will be modifying or creating new cookiecutters (you can have multiple templates that are called from the command line).&lt;/p&gt;

&lt;p&gt;I‚Äôm looking forward to my research workflow being refined over time and becoming more robust and efficient‚Äîhopefully the process described above is a good starting point.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="workflow" /><category term="yearofml" /><summary type="html">There are two categories of projects that I‚Äôve collected: ML papers and tutorials/working examples. For both of these I want a consistent workflow that allows me to present summaries in an quick and efficient manner and to reference in the future.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">A Year of Machine Learning, Neural Networks, and more</title><link href="https://www.cpdis.com/Year-of-Learning/" rel="alternate" type="text/html" title="A Year of Machine Learning, Neural Networks, and more" /><published>2018-01-01T14:38:50-06:00</published><updated>2018-01-01T14:38:50-06:00</updated><id>https://www.cpdis.com/Year-of-Learning</id><content type="html" xml:base="https://www.cpdis.com/Year-of-Learning/">&lt;p&gt;My goal this year, and with this project, is to read, replicate, and expand upon as many different papers, tutorials, and Github repositories as possible. Each week I plan on going through all the &lt;a href=&quot;https://github.com/cpdis/Experiments&quot;&gt;links&lt;/a&gt; and tabs I‚Äôve collected over the months and posting whatever the results are here. In addition, I want to improve my writing skills and the only way to do that is by practice and repetition. Please forgive my writing the first few months, it will, it has to, get better.&lt;/p&gt;

&lt;p&gt;It‚Äôs relatively easy (ü§ì) to take online &lt;a href=&quot;https://www.udacity.com/course/intro-to-machine-learning--ud120&quot;&gt;classes&lt;/a&gt;, &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;courses&lt;/a&gt;, and &lt;a href=&quot;https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013&quot;&gt;nanodegrees&lt;/a&gt; but getting your hands dirty and working through problems on your own is where real learning begins. I‚Äôm very aware that I‚Äôm probably not going to be leading authority in the field of artificial intelligence (or even someone worth a &lt;a href=&quot;https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html&quot;&gt;seven figure&lt;/a&gt; salary), that was a decision I should have made fifteen years ago. However, becoming proficient/efficient at implementing models and being up-to-date with the latest research is a huge step in the right direction.&lt;/p&gt;

&lt;p&gt;In this first week of January I‚Äôm going to try to nail down the workflow that I use to actually complete this project and to document what I learn. I have a local folder ready for any code or papers that need to be stored locally and a &lt;a href=&quot;http://www.amazon.com/dp/B075N1Z9LT/?tag=heismukamily-20&quot;&gt;NAS&lt;/a&gt; to store large datasets. Everything will be on Github for version control and to show/share my work (and so I can get more practice using Git). I‚Äôm going to be using my 2014 13‚Äù MacBook Pro for most of this project so there definitely won‚Äôt be any training time records set. In my &lt;a href=&quot;https://github.com/cpdis/P3_CarND_Behavioral_Cloning&quot;&gt;experience&lt;/a&gt;, though, simple models run quickly and long training times may incentive me to &lt;a href=&quot;https://navoshta.com/meet-fenton/&quot;&gt;upgrade my machine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The pace at which deep learning and artificial intelligence advances are being made (no matter how specialized) is truly astounding. I, for one, welcome our new ü§ñ overlords.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="machine learning" /><category term="deep learning" /><category term="artificial intelligence" /><category term="learning" /><summary type="html">My goal this year, and with this project, is to read, replicate, and expand upon as many different papers, tutorials, and Github repositories as possible. Each week I plan on going through all the links and tabs I‚Äôve collected over the months and posting whatever the results are here. In addition, I want to improve my writing skills and the only way to do that is by practice and repetition. Please forgive my writing the first few months, it will, it has to, get better.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">The Parable of the iPhone</title><link href="https://www.cpdis.com/The-Parable-of-the-iPhone/" rel="alternate" type="text/html" title="The Parable of the iPhone" /><published>2017-12-31T14:55:01-06:00</published><updated>2017-12-31T14:55:01-06:00</updated><id>https://www.cpdis.com/The-Parable-of-the-iPhone</id><content type="html" xml:base="https://www.cpdis.com/The-Parable-of-the-iPhone/">&lt;p&gt;A 15 year old girl was picking berries from a bush one day in ancient Rome. And so it happened right before her eyes that a strange and mysterious wormhole opened up and presented an iPhone 7 Plus. She gasped in shock at the incomprehensible sight that had materialized out of thin air right before her eyes.&lt;/p&gt;

&lt;p&gt;Was it a precious metal of some kind? Perhaps a gem or a stone. It didn‚Äôt look like anything she had ever seen before. Could it be an exotic plant of some kind? A strange animal? Perhaps an extremely advanced tool. She reached her hand out to touch it. But then thought better of it. She better get her sister to take a look at it as well.&lt;/p&gt;

&lt;p&gt;She called out. Her sister was nearby and immediately ran over.&lt;/p&gt;

&lt;p&gt;‚ÄúLook at this!‚Äù, she said.&lt;/p&gt;

&lt;p&gt;Her sister‚Äôs face looked bewildered, ‚ÄúWhat is it?‚Äù&lt;/p&gt;

&lt;p&gt;‚ÄúI don‚Äôt know, it just appeared.‚Äù&lt;/p&gt;

&lt;p&gt;The sister picked up the phone. She marveled at its simplicity with affection. The vibrant silver color and precise lines of its edges. The small foreign writing on the metallic back with an image engraved in the finest handiwork of an apple that had a bite taken out.&lt;/p&gt;

&lt;p&gt;‚ÄúThis is magnificent‚Äù she said as she touched a part of the object that had a small contusion in the otherwise perfectly smooth surface. When she touched it, a brilliant light shone from the face of the object, like magic.&lt;/p&gt;

&lt;p&gt;The girls were awestruck. They thought they had better show this to their mother.&lt;/p&gt;

&lt;p&gt;A few hours later, the iPhone 7 Plus was in the hands of a Roman centurion who was taking a slow-mo video of his own face when the object disappeared as suddenly as it had arrived.&lt;/p&gt;

&lt;p&gt;A similar thing happened in two other places at two other times. The wormhole had opened. The iPhone had appeared. Once in South America to a young man in a hunter gatherer society and once to a middle aged woman in Manila in 1968. The result of these three temporary appearances of the iPhone 7 Plus shook the cultures that the iPhone had presented itself to to the core. Each had named the events and the object in different ways.&lt;/p&gt;

&lt;p&gt;The Romans had interpreted the event as an act of Caesar to display his power. The phone ended up being named the Delustricus. They started a holiday to celebrate the gracious revelation their Lord had bestowed upon them.&lt;/p&gt;

&lt;p&gt;The South American hunter gatherers had named the object what can be translated roughly as Sun Jewel, they ended up worshipping the Sun Jewel as their prime deity. Sacrifice by fire was the deity‚Äôs choice of preferred worship.&lt;/p&gt;

&lt;p&gt;The Filipino‚Äôs that had encountered the phone had eventually come to the conclusion that the device was an alien machine of infinite power that they called the Nuckacot. It happened to appear on a Tuesday precisely while the woman who had discovered it was overcooking a pot of rice. The culture had developed a custom of overcooking rice on Tuesdays in order to appease the aliens and pray for the return of the mysterious and beautiful Nuckacot.&lt;/p&gt;

&lt;p&gt;You see, this is what human beings do when we don‚Äôt have words for something, we make up words for them. Even if the thing we are trying to talk about is not a thing, is beyond thingness, but an experience that transcends all of our language and conceptions. We still try to find words, we still try to make meaning and tell stories. And so we get religion with all of its different variations and forms and practices.&lt;/p&gt;

&lt;p&gt;Are all religions the same? No, of course not.&lt;/p&gt;

&lt;p&gt;Neither are all the explanations of the iPhone. They are actually quite different.&lt;/p&gt;

&lt;p&gt;But is the iPhone the same? Of course it is.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="apple" /><category term="technology" /><category term="iphone" /><category term="religion" /><summary type="html">A 15 year old girl was picking berries from a bush one day in ancient Rome. And so it happened right before her eyes that a strange and mysterious wormhole opened up and presented an iPhone 7 Plus. She gasped in shock at the incomprehensible sight that had materialized out of thin air right before her eyes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry></feed>