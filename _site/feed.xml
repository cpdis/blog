<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://www.cpdis.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.cpdis.com/" rel="alternate" type="text/html" /><updated>2018-02-16T10:27:22-06:00</updated><id>https://www.cpdis.com/</id><title type="html">Colin Dismuke</title><subtitle>A blog about AI, cryptoassets, and running.</subtitle><entry><title type="html">Breaking a CAPTCHA in 15 Minutes</title><link href="https://www.cpdis.com/Breaking-a-CAPTCHA-in-15-Minutes/" rel="alternate" type="text/html" title="Breaking a CAPTCHA in 15 Minutes" /><published>2018-02-16T10:17:50-06:00</published><updated>2018-02-16T10:17:50-06:00</updated><id>https://www.cpdis.com/Breaking-a-CAPTCHA-in-15-Minutes</id><content type="html" xml:base="https://www.cpdis.com/Breaking-a-CAPTCHA-in-15-Minutes/">&lt;h1 id=&quot;breaking-a-captcha-in-15-minutes&quot;&gt;Breaking a CAPTCHA in 15 Minutes&lt;/h1&gt;

&lt;p&gt;So, this took me a lot longer to get to than I was planning on ü§¶üèº‚Äç‚ôÇÔ∏è Despite my best intentions and greatest ambitions, life continues to happen and usually take priority over projects I‚Äôm doing on the side. The irony is that this project is relatively simple, both &lt;a href=&quot;https://twitter.com/ageitgey&quot;&gt;Adam Geitgey‚Äôs&lt;/a&gt; original project/code and my implementation of it.&lt;/p&gt;

&lt;p&gt;For this project, and others like it, I‚Äôve decided the best way to learn (and present what I‚Äôve learned) is to go through the code, mostly line by line and explain what is going on. There are definitely opportunities to modify/improve the original code, use different training data, and test out different neural network implementations, but in the interest of time I‚Äôm just going to leave everything as is.&lt;/p&gt;

&lt;p&gt;The tools being used for this project are Python 3, OpenCV, Keras, Tensorflow. Pretty much your standard deep learning stack with OpenCV being used for image augmentation and manipulation. OpenCV was used extensively in the &lt;a href=&quot;https://github.com/cpdis/P5_CarND_Vehicle_Detection_and_Tracking&quot;&gt;first term&lt;/a&gt; of the Udacity Self Driving Car Nanodegree.&lt;/p&gt;

&lt;p&gt;The CAPTCHA training data was ‚Äúcollected‚Äù by hacking around with the WordPress CAPTCHA plug-in and outputting the CAPTCHA images along with their correct filenames.&lt;/p&gt;

&lt;p&gt;It‚Äôs definitely possible to train a neural network on an image containing more than one letter/numeral, however, if it‚Äôs possible to split them up the accuracy should increase and the training time decrease. This is exactly what was done with OpenCV in this project üëåüèº&lt;/p&gt;

&lt;p&gt;The process for extracting the single letters, training the neural network, and using the model to solve CAPTCHAs is straightforward.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Extract single letters from the CAPTCHA images.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python extract_single_letters_from_captchas.py&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train the neural network to recognize single letters.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python train_model.py&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use the model to solve the CAPTCHAs based on the trained model.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python solve_captchas_with_model.py&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I‚Äôll go through each of these steps, explain the code and any problems that I encountered, and show the results.&lt;/p&gt;

&lt;h2 id=&quot;-extract-single-letters-for-the-captcha-images&quot;&gt;‚õè Extract single letters for the CAPTCHA images&lt;/h2&gt;

&lt;p&gt;The first step in this process is leveraging the power of OpenCV to split the CAPTCHA images into separate letters and then augment the resulting letters for optimal learning. In order to process all of the images, we need a list of all of the images in &lt;code class=&quot;highlighter-rouge&quot;&gt;CAPTCHA_IMAGE_FOLDER&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;captcha_image_files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CAPTCHA_IMAGE_FOLDER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;glob&lt;/code&gt; finds all the pathnames that match a specified pattern while &lt;code class=&quot;highlighter-rouge&quot;&gt;join&lt;/code&gt; joins together two or more pathname components. The &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; is equivalent to all files in the directory. So, putting that all together, &lt;code class=&quot;highlighter-rouge&quot;&gt;glob&lt;/code&gt; is getting all of the pathnames inside of the &lt;code class=&quot;highlighter-rouge&quot;&gt;CAPTCHA_IMAGE_FOLDER&lt;/code&gt; and saving them to the list &lt;code class=&quot;highlighter-rouge&quot;&gt;captcha_image_files&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next, we loop over all of the images and perform the image augmentation. &lt;code class=&quot;highlighter-rouge&quot;&gt;enumerate()&lt;/code&gt; is used so that each of the image filenames is assigned an index. In this case, the index is only used to show the progress of the image processing.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;captcha_image_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;captcha_image_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[INFO] processing image {}/{}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;captcha_image_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;basename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;captcha_image_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;captcha_correct_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splitext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;captcha_image_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copyMakeBorder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BORDER_REPLICATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;thresh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;THRESH_BINARY_INV&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;THRESH_OTSU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each image is loaded using &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.imread()&lt;/code&gt; and converted into grayscale with &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)&lt;/code&gt;. An 8 pixel border is added around each letter using &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)&lt;/code&gt; so that the image isn‚Äôt constrained to the edges of the letters. &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.BORDER_REPLICATE&lt;/code&gt; replicates the pixels on the edge of the image to create the border. There are &lt;a href=&quot;https://docs.opencv.org/3.1.0/d3/df2/tutorial_py_basic_ops.html&quot;&gt;several&lt;/a&gt; other options for creating a border in OpenCV as well. The final step before isolating the individual letters is &lt;em&gt;thresholding&lt;/em&gt; the full image using &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]&lt;/code&gt;. Thresholding makes it easier to find the continuous regions that make up each letter.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;contours&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findContours&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thresh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RETR_EXTERNAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CHAIN_APPROX_SIMPLE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;contours&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contours&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imutils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contours&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Conveniently, OpenCV has a built-in function, &lt;code class=&quot;highlighter-rouge&quot;&gt;findcontours()&lt;/code&gt;, that finds continuous regions of pixels of the same color. &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.RETR_EXTERNAL&lt;/code&gt; is a flag used to only extract the outer contours of the region while &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.CHAIN_APPROX_SIMPLE&lt;/code&gt; removes all redundant points and compresses the contour, saving memory.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contour&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contours&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boundingRect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;half_width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;letter_image_regions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;half_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;letter_image_regions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;half_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;half_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;letter_image_regions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The next snippet checks to make sure that the letters aren‚Äôt conjoined. This happens some times in CAPTCHAs when the letters are very close to each other. Easy for a human to recognize, a little more complicated when you‚Äôre using computer vision. To get around this problem, the height and width of the contour are examined and those that have a width to height ratio greater than 1.25 are split down the middle.&lt;/p&gt;

&lt;p&gt;The detected letter images are then sorted using &lt;code class=&quot;highlighter-rouge&quot;&gt;letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;lambda&lt;/code&gt; effectively creates an inline function instead of a named function that returns the sorted first elements of the array.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;element_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;letter_image_regions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above is reduced to one line and reduces the code complexity.&lt;/p&gt;

&lt;p&gt;Finally, each of the letters is saved as a single image. &lt;code class=&quot;highlighter-rouge&quot;&gt;zip()&lt;/code&gt; returns a list of tuples, in this case the &lt;code class=&quot;highlighter-rouge&quot;&gt;letter_image_regions&lt;/code&gt; and the &lt;code class=&quot;highlighter-rouge&quot;&gt;captcha_correct_text&lt;/code&gt;. Each different letter is saved to its own folder so that they are easily accesible and organized.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;letter_bounding_box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;letter_text&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;letter_image_regions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;captcha_correct_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;letter_bounding_box&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;letter_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OUTPUT_FOLDER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;letter_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;letter_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;{}.png&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zfill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;letter_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;-train-the-neural-network&quot;&gt;üèãüèº‚Äç Train the neural network&lt;/h2&gt;

&lt;p&gt;Since recognizing letters and numbers is a relatively simple tasked compared to more complex images like dogs, cats, and roads; a complex neural network architecture isn‚Äôt needed. A simple convolutional neural network with two convolutional layers and two fully connected layers is more than sufficient.&lt;/p&gt;

&lt;p&gt;Before the network is trained a few things need to be done. The individual letter images are converted to grayscale, resized with consistent dimensions (20x20 in this case), and a third channel (color) needs to be added to avoid Keras errors. The label for each image is then created from the name of the folder that the image was located in. We end up with &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;labels&lt;/code&gt; arrays that we will use for training.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_file&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LETTER_IMAGES_FOLDER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resize_to_fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The image data is then &lt;a href=&quot;https://en.wikipedia.org/wiki/Normalization_(image_processing)&quot;&gt;normalized&lt;/a&gt; so that the pixel intensity is between 0 and 1.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;float&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; function &lt;code class=&quot;highlighter-rouge&quot;&gt;train_test_split&lt;/code&gt; is then used to create training and test sets. &lt;code class=&quot;highlighter-rouge&quot;&gt;test_size&lt;/code&gt; refers to the proportion of the data that is used in the test split.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since there are 32 different letters and numbers being classified standard one-hot encoding won‚Äôt work. This is remedied using the &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; function &lt;a href=&quot;http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelBinarizer.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. It allows you to have one regressor or binary classifier per class (letter or number).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelBinarizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using Keras the neural network only take a few lines of code to implement. A &lt;code class=&quot;highlighter-rouge&quot;&gt;Sequential&lt;/code&gt; model in Keras is simply a linear stack of layers. In order, the first &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2D&lt;/code&gt; layer consists of 20 filters, a &lt;code class=&quot;highlighter-rouge&quot;&gt;5x5&lt;/code&gt; filter window, &lt;code class=&quot;highlighter-rouge&quot;&gt;same&lt;/code&gt; padding (which means that output size is the same as the input size and requires the filter window to slip outside of the input, requiring padding), input shape of 20x20x1 (since the images are 20 pixels square and have one color channel), and use the &lt;code class=&quot;highlighter-rouge&quot;&gt;relu&lt;/code&gt; activation (rectified linear unit). The &lt;code class=&quot;highlighter-rouge&quot;&gt;MaxPooling2D&lt;/code&gt; layer uses the most common form which downsamples every depth slice in the input by 2 along both the width and height axes, discarding 75% of the activations. The second &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2D&lt;/code&gt; layer uses the same parameters but increases the number of filters and the second &lt;code class=&quot;highlighter-rouge&quot;&gt;MaxPooling2D&lt;/code&gt; layer is the same as the first pooling layer. The &lt;code class=&quot;highlighter-rouge&quot;&gt;Flatten&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Dense&lt;/code&gt; layers create a hidden layer with 500 nodes. The final &lt;code class=&quot;highlighter-rouge&quot;&gt;Dense&lt;/code&gt; layer is the output with 32 nodes for each of the possible letters and numbers.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;same&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;same&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;softmax&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Graphically, the neural network looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-02-18/CAPTCHA_model.png&quot; alt=&quot;Neural network model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model is compiled in a single line using Keras. There are many types of &lt;a href=&quot;https://keras.io/losses/&quot;&gt;losses&lt;/a&gt; and &lt;a href=&quot;https://keras.io/optimizers/&quot;&gt;optimizers&lt;/a&gt; but cross entropy and the Adam optimizer are two of the most commonly used.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;categorical_crossentropy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;adam&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the model is trained on the training data and validated on the test data that was created earlier.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After 10 epochs the accuracy is almost 100%, even after 1 epoch the accuracy is 99.5%. Given that this is a relatively simple classification task even higher accuracy is definitely achievable. Training on the command line looked like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Using TensorFlow backend.
Train on 29058 samples, validate on 9686 samples
Epoch 1/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 41s 1ms/step - loss: 0.2413 - acc: 0.9413 - val_loss: 0.0226 - val_acc: 0.9950
Epoch 2/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 44s 2ms/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.0140 - val_acc: 0.9968
Epoch 3/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 46s 2ms/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0081 - val_acc: 0.9977
Epoch 4/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 40s 1ms/step - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0054 - val_acc: 0.9988
Epoch 5/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 38s 1ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0115 - val_acc: 0.9975
Epoch 6/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 37s 1ms/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0076 - val_acc: 0.9979
Epoch 7/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 39s 1ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0129 - val_acc: 0.9971
Epoch 8/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 38s 1ms/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0177 - val_acc: 0.9947
Epoch 9/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 41s 1ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0047 - val_acc: 0.9991
Epoch 10/10
29058/29058 &lt;span class=&quot;o&quot;&gt;[==============================]&lt;/span&gt; - 42s 1ms/step - loss: 2.3045e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9991
The elapsed training &lt;span class=&quot;nb&quot;&gt;time &lt;/span&gt;is:  429.6541633605957
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The total training time was 430 seconds or 7 minutes and 10 seconds.&lt;/p&gt;

&lt;h2 id=&quot;-solve-the-captchas&quot;&gt;ü§ì Solve the CAPTCHAs&lt;/h2&gt;

&lt;p&gt;Now that the model is trained the last step is actually looking at the CAPTCHA images and classifying the letters. The trained model data is loaded and 10 random CAPTCHA files are chosen.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_LABELS_FILENAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_FILENAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;captcha_image_files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CAPTCHA_IMAGE_FOLDER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;captcha_image_files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;captcha_image_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;model.predict()&lt;/code&gt; outputs a one-hot encoded prediction of what the letter or number should be classified as. The one-hot encoded output is then transformed back to a nornal letter using &lt;code class=&quot;highlighter-rouge&quot;&gt;lb.inverse_transform()&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;inverse_transform()&lt;/code&gt; is a method of LabelBinarizer that allows you to transform binary labels back to multi-class labels.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;letter_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;letter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inverse_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;letter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-02-18/CLI_output.jpg&quot; alt=&quot;CLI Output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to the prediction being output to the command line, the CAPTCHA image with the predicted letters/numbers is shown using &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.imshow()&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.waitKey()&lt;/code&gt; either waits for a keystroke to advance to the next image or a specified amount of time (500-ms in this case).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Wait for a specified amount of time (in ms) before moving to the next image&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-02-18/CAPTCHA_output.jpg&quot; alt=&quot;CAPTCHA Output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The slightly modified code, resources, and notes for this project are located &lt;a href=&quot;https://github.com/cpdis/Experiments/tree/master/2_Breaking_CAPTCHA&quot;&gt;here&lt;/a&gt;. The original code and data can be downloaded &lt;a href=&quot;https://s3-us-west-2.amazonaws.com/mlif-example-code/solving_captchas_code_examples.zip&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="Convolutional" /><category term="CNN" /><category term="Keras" /><category term="OpenCV" /><category term="yearofML" /><summary type="html">Breaking a CAPTCHA in 15 Minutes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">Setting up my Machine Learning Environment</title><link href="https://www.cpdis.com/Setting-up-my-Machine-Learning-Environment/" rel="alternate" type="text/html" title="Setting up my Machine Learning Environment" /><published>2018-01-27T10:47:39-06:00</published><updated>2018-01-27T10:47:39-06:00</updated><id>https://www.cpdis.com/Setting-up-my-Machine-Learning-Environment</id><content type="html" xml:base="https://www.cpdis.com/Setting-up-my-Machine-Learning-Environment/">&lt;p&gt;As I mentioned in my last post, I unfortunately lost my machine learning environment that I had set up on my laptop. Fortunately, though, getting up and running with Python, Keras, and Tensorflow is quite easy. All of the individual packages that you might want to use for machine/deep learning can be downloaded, cloned, acquired individually but it seems like the standard (and easiest for beginners) method is to use a distribution like &lt;a href=&quot;https://www.continuum.io/downloads&quot;&gt;Anaconda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anaconda is a leading open data science platform that‚Äôs powered by Python. I‚Äôll be using the open source version but there‚Äôs also an &lt;a href=&quot;https://www.anaconda.com/enterprise/&quot;&gt;enterprise&lt;/a&gt; distribution as well. The major benefit of using a distribution is that out of the box it includes over 100 (read &lt;em&gt;most&lt;/em&gt;) of the most popular Python and R packages for data science.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-conda-environment&quot;&gt;Creating a conda environment&lt;/h2&gt;
&lt;p&gt;Also included with Anaconda is a versatile package manager call &lt;code class=&quot;highlighter-rouge&quot;&gt;conda&lt;/code&gt;. Like &lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt;, it quickly installs, runs, and updates packages and their dependencies. You can also use it to search the package index, create new environments (the main point of this post), and install packages into existing environments.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;conda&lt;/code&gt; environments are collections of packages with specific versions that can be used to ensure the portability of Python code. This is useful when collaborating with someone or using two different versions of a package simultaneously. I created two machine learning environments, &lt;code class=&quot;highlighter-rouge&quot;&gt;ML2&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ML3&lt;/code&gt;. Based off of Python 2.7.14 and 3.6.4, respectively. It only took a few simple steps to create both of these environments.&lt;/p&gt;

&lt;p&gt;In Terminal type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create -n ML2 python=2.7.14 pandas scikit-learn jupyter matplotlib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and hit &lt;code class=&quot;highlighter-rouge&quot;&gt;Enter&lt;/code&gt;. When prompted, answer &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; and you should see this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# To activate this environment, use:
# &amp;gt; source activate ML2
#
# To deactivate this environment, use:
# &amp;gt; source deactivate ML2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Activating the environment is as simple as typing:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source activate ML2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can see that the environment is activated because the name &lt;code class=&quot;highlighter-rouge&quot;&gt;(ML2)&lt;/code&gt; is prepended to your prompt.&lt;/p&gt;

&lt;h2 id=&quot;install-tensorflow-and-keras&quot;&gt;Install Tensorflow and Keras&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; is an open source software library for numerical computation using data flow graphs. The way that it is built allows it to be used on one or more CPUs, GPUs, servers, or phones with a single API. It was originally developed by researchers and engineers on the Google Brain Team.&lt;/p&gt;

&lt;p&gt;To install TensorFlow, make sure the environment you want to use is active, in this case &lt;code class=&quot;highlighter-rouge&quot;&gt;ML2&lt;/code&gt; and type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The other super useful package is &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt;, a high-level neural network specification implemented in Python that runs on top of TensorFlow. It makes creating a model much simpler than using TensorFlow which is paramount for fast experimentation.&lt;/p&gt;

&lt;p&gt;To install Keras, make sure the environment you want to use is active, in this case &lt;code class=&quot;highlighter-rouge&quot;&gt;ML2&lt;/code&gt; and type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install keras
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;test-it&quot;&gt;Test it!&lt;/h2&gt;

&lt;p&gt;To make sure everything is running smoothly type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipython
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which will bring up the iPython console and display:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Python 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:05:08) 
Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -&amp;gt; Introduction and overview of IPython's features.
%quickref -&amp;gt; Quick reference.
help      -&amp;gt; Python's own help system.
object?   -&amp;gt; Details about 'object', use 'object??' for extra details. details.

In [1]:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Import TensorFlow and Keras:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow, keras
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which should result in the following output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Using TensorFlow backend.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These aren‚Äôt the only tools that I‚Äôll be using but they are a solid base. Distributions such as Anaconda and package managers like conda, pip, and homebrew make adding new tools extremely easy.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="machine learning" /><category term="Anaconda" /><category term="tensorflow" /><category term="keras" /><category term="yearofML" /><summary type="html">As I mentioned in my last post, I unfortunately lost my machine learning environment that I had set up on my laptop. Fortunately, though, getting up and running with Python, Keras, and Tensorflow is quite easy. All of the individual packages that you might want to use for machine/deep learning can be downloaded, cloned, acquired individually but it seems like the standard (and easiest for beginners) method is to use a distribution like Anaconda.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">Lost Environment and FloyHub</title><link href="https://www.cpdis.com/Lost-Environment-and-FloydHub/" rel="alternate" type="text/html" title="Lost Environment and FloyHub" /><published>2018-01-24T15:32:35-06:00</published><updated>2018-01-24T15:32:35-06:00</updated><id>https://www.cpdis.com/Lost-Environment-and-FloydHub</id><content type="html" xml:base="https://www.cpdis.com/Lost-Environment-and-FloydHub/">&lt;p&gt;It turns out that at some point in the past few months the environment that I had setup for machine learning (&lt;a href=&quot;https://www.tensorflow.org&quot;&gt;Tensorflow&lt;/a&gt;, &lt;a href=&quot;https://keras.io&quot;&gt;Keras&lt;/a&gt;, &lt;a href=&quot;http://opencv-python-tutroals.readthedocs.io/en/latest/&quot;&gt;OpenCV&lt;/a&gt;, etc.) was somehow lost in the ether.&lt;/p&gt;

&lt;p&gt;How could this happen you might ask? I used &lt;a href=&quot;https://www.anaconda.com&quot;&gt;Anaconda&lt;/a&gt; to manage the different environments I had (&lt;code class=&quot;highlighter-rouge&quot;&gt;source activate myenv&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;source deactivate myenv&lt;/code&gt;) and I recently did a clean install of Python to clear up some issues I was having with regard to &lt;a href=&quot;https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013&quot;&gt;Udacity‚Äôs Self Driving Car Nanodegree&lt;/a&gt;. Because of this, when I went to start on the project for this week, &lt;a href=&quot;https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710&quot;&gt;Breaking a CAPTCHA in 15 minutes&lt;/a&gt;, I realized that none of the packages I had previously installed were there. Not good. Fortunately, I like starting things from scratch and this gives me the opportunity to write a post about the way that I setup my environment.&lt;/p&gt;

&lt;p&gt;While I was researching how different people have set up their machine learning environments I came across FloydHub. FloydHub is one of the newish class of platform-as-a-service (PaaS) services. They basically allow you to train and deploy deep learning models in the cloud with just a few commands and parameters on the command line. Most of the popular frameworks are available (&lt;code class=&quot;highlighter-rouge&quot;&gt;floyd run --env tensorflow&lt;/code&gt;) and a range of hardware is available at relatively affordable prices at per second rates.&lt;/p&gt;

&lt;p&gt;One of the highlights of FloydHub (setting aside making the whole process almost frictionless) is the ability to create a Jupyter notebook for the project when you‚Äôre starting your model. Follow along and see how easy it is to get a model running:&lt;/p&gt;

&lt;h3 id=&quot;1-create-a-floydhub-account&quot;&gt;1. Create a FloydHub account&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.floydhub.com/signup&quot;&gt;Sign up&lt;/a&gt; on FloydHub&lt;/li&gt;
  &lt;li&gt;Install the floyd CLI on your local machine through these two &lt;a href=&quot;https://www.floydhub.com/welcome&quot;&gt;steps&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip install -U floyd-cli

$ floyd login
# Follow the instructions on your CLI
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Upon logging in, you will need to copy and paste a private key into the command line that authenticates your machine with the service. FloydHub provides a nice UX for copying that üîë during the account creation process‚Äîit‚Äôs the little things that make you like a product.&lt;/p&gt;

&lt;h3 id=&quot;2-clone-this-project-to-your-local-machine&quot;&gt;2. Clone this project to your local machine&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd /path/to/your-project-dir
$ floyd clone experiments/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-create-your-project-version-on-floydhub&quot;&gt;3. Create your project version on FloydHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.floydhub.com/projects/create&quot;&gt;Create a project&lt;/a&gt; on FloydHub and then sync the cloned repository with your new project:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ floyd init your-project-name&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-run-the-project-through-a-jupyter-notebook&quot;&gt;4. Run the project through a jupyter notebook&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;--env&lt;/code&gt; flag specifies the environment that this project should run on.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;--data&lt;/code&gt; flag specifies the dataset that should be available in the &lt;code class=&quot;highlighter-rouge&quot;&gt;/data&lt;/code&gt; directory and the models should be available at the &lt;code class=&quot;highlighter-rouge&quot;&gt;/models&lt;/code&gt; directory&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;--mode&lt;/code&gt; flag specifies that the job should create a Jupyter notebook.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;--gpu&lt;/code&gt; flag specifies whether the GPU should be used to accelerate the training process.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;floyd run \
  --env tensorflow:py2 \
  --data experiments/datasets/üê∂/2:data \
  --data experiments/datasets/üê∂-models/1:models \
  --mode jupyter \
  --gpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the job is started, the jupyter notebook will open in your browser and you are ready to go!&lt;/p&gt;

&lt;p&gt;The next post will be about setting up my machine learning environment followed by the project that I was tentatively supposed to be finished with a few days ago.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="python" /><category term="anaconda" /><category term="floydhub" /><category term="yearofml" /><summary type="html">It turns out that at some point in the past few months the environment that I had setup for machine learning (Tensorflow, Keras, OpenCV, etc.) was somehow lost in the ether.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">PathNet - Evolution Channels Gradient Descent in Super Neural Networks</title><link href="https://www.cpdis.com/PathNet-A-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/" rel="alternate" type="text/html" title="PathNet - Evolution Channels Gradient Descent in Super Neural Networks" /><published>2018-01-17T13:40:10-06:00</published><updated>2018-01-17T13:40:10-06:00</updated><id>https://www.cpdis.com/PathNet-A-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks</id><content type="html" xml:base="https://www.cpdis.com/PathNet-A-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/">&lt;p&gt;&lt;em&gt;Note: Inevitably, this past week was much more busy than planned. As such, I didn‚Äôt get to explore much more than the paper itself and the Github repository. I‚Äôd rather not do the minimum amount of work to achieve a goal but in this case it‚Äôs the best I can do.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I first came across PathNet in Azeem Azhar‚Äôs essential &lt;a href=&quot;https://www.getrevue.co/profile/azeem?utm_campaign=Issue&amp;amp;utm_content=forwarded&amp;amp;utm_medium=email&amp;amp;utm_source=Azeem+Azhar%3A+The+Exponential+View&quot;&gt;The Exponential View&lt;/a&gt; newsletter almost exactly a year ago. DeepMind was causing a stir in the AI community because PathNet was a plausible precursor to an architecture that could support artificial general intelligence (AGI). PathNet combines modular deep learning, meta-learning, and reinforcement learning and is summarized this way in the introduction to the &lt;a href=&quot;https://arxiv.org/abs/1701.08734&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Neural networks, in general, are trained on data for each specific task they are trying to achieve. This is time consuming and not efficient. Transfer learning was developed to bypass this problem but has limited use. PathNet goes beyond transfer learning, where knowledge gained while solving one problem is applied to a different but related problem, it finds the best parameters to be reused for transfer learning and implements those. Essentially, a neural network of neural networks.&lt;/p&gt;

&lt;p&gt;A PathNet is a modular deep neural network with any number of layers consisting of modules. Each module within each layer is itself a neural network (see last sentence of the previous paragraph). Each neural network module is either convolutional or linear and is followed by a transfer function (RELUs in this case). At each layer the output of each module is summed and then passed on to the next layer. While there may be an arbitrary number of modules per layer, typically a maximum of 3 or 4 distinct modules are permitted in the final pathway. The final layer in a PathNet is unique and not shared between different tasks. The figure belows shows this model in action. The first three layers are convolutional 2D kernels with 8 kernels per module (the green boxes in the figure), kernel sizes of (8, 4, 3), and strides (4, 2, 1) from the first to the third layer, respectively. After each module is a RELU and the layers are summed before being passed on to the next layer (light blue boxes). The red boxes show the modules that are passed on to the next layer, if all modules were included then as the model evolved it would simply grow to encompass the entire network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/atari.jpeg&quot; alt=&quot;Atari&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The tasks that were considered were MNIST classification, CIFAR and SVHN, several Atari games, and several Labyrinth games. For binary MNIST classification the researchers found that PathNet helped speed up learning in the classification task by decreasing the mean time to solution from 229 generations to 167 generations. They found this to be the case for both the control (independent learning) and when the hyperparameters were fine tuned. The speedup ratio compared to independent learning was 1.18. The histograms below clearly show the reduction in the number of generations to achieve 0.998 accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/MNIST.jpeg&quot; alt=&quot;MNIST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moving on to the Atari games, the researchers found that PathNet was superior to fine-tuning. fine-tuning was performed by doing a hyperparameter sweep of learning rates and entropy costs while PathNet was investigated using a range of evaluation times, mutation rates, and tournament sizes. [I understand that it‚Äôs necessary to tune the model to achieve optimal results, however, if you must tune PathNet doesn‚Äôt that make it a little less viable as AGI?] An optimal combination of tournament size and mutation rate were found for PathNet that achieved rapid convergence and a speedup ratio of 1.33 versus 1.16 for fine-tuning. The figure below shows the results for the first 40 million steps of training for PathNet (blue), fine-tuning (green), and independent learning (red). The results for both PathNet and fine-tuning show the top five hyperparameter settings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/atari2.jpeg&quot; alt=&quot;Atari2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, three labyrinth games were tested, &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;stairway_to_melon&lt;/code&gt;. All of the games are part of DeepMind‚Äôs &lt;a href=&quot;https://github.com/deepmind/lab&quot;&gt;DeepMind Lab&lt;/a&gt;. Again, a hyperparameter sweep was used for fine-tuning; mutation rates, module duplication rates, and tournament size were varied while learning rate, entropy cost, and evaluation time were fixed. PathNet learns the second task faster than fine tuning for transfer to &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt; and transfer from &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt;. PathNet also performs better when learning &lt;code class=&quot;highlighter-rouge&quot;&gt;stairway_to_melon&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt; from scratch. Interestingly, when transferring to &lt;code class=&quot;highlighter-rouge&quot;&gt;lt_chasm&lt;/code&gt;, both fine tuning and PathNet perform worse than independent learning. Speedup for PathNet is 1.26 versus 1.0 for fine-tuning (this is skewed by the good performance of transferring from &lt;code class=&quot;highlighter-rouge&quot;&gt;seekavoid_arena&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;stairway_to_melon&lt;/code&gt;). The figure below shows the mean of the five best training runes for PathNet compared with fine-tuning (the off diagonal plots) and independent learning (diagonal plots labeled from scratch). The results are more mixed than the previous examples, however, in most cases PathNet performs better than the control. especially when transferring from one game to another.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/16-01-18/labyrinth.jpeg&quot; alt=&quot;Labyrinth&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It‚Äôs pretty clear that PathNet represents a step toward AGI. I wish that I had more time to look at the &lt;a href=&quot;https://github.com/jaesik817/pathnet&quot;&gt;code&lt;/a&gt;, play with it, and see it in action with some of the examples from the paper but I‚Äôm unfortunately already behind with this project.&lt;/p&gt;

&lt;p&gt;The code, notes, and reference files for this week are in this &lt;a href=&quot;https://github.com/cpdis/Experiments/tree/master/1_DeepMind_PathNet&quot;&gt;repository&lt;/a&gt;.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="transfer learning" /><category term="evolution learning" /><category term="continual learning" /><category term="giant networks" /><category term="yearofml" /><summary type="html">Note: Inevitably, this past week was much more busy than planned. As such, I didn‚Äôt get to explore much more than the paper itself and the Github repository. I‚Äôd rather not do the minimum amount of work to achieve a goal but in this case it‚Äôs the best I can do.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">Folder Structure and Workflow</title><link href="https://www.cpdis.com/Folder-Structure-and-Workflow/" rel="alternate" type="text/html" title="Folder Structure and Workflow" /><published>2018-01-08T22:00:18-06:00</published><updated>2018-01-08T22:00:18-06:00</updated><id>https://www.cpdis.com/Folder-Structure-and-Workflow</id><content type="html" xml:base="https://www.cpdis.com/Folder-Structure-and-Workflow/">&lt;p&gt;There are two categories of projects that I‚Äôve collected: ML papers and tutorials/working examples. For both of these I want a consistent workflow that allows me to present summaries in an quick and efficient manner and to reference in the future.&lt;/p&gt;

&lt;h2 id=&quot;papers&quot;&gt;Papers&lt;/h2&gt;
&lt;p&gt;I think that I have come up with a decent template for presenting a summary of each of the papers that I read as well as describe how they might be useful. There will be four sections: &lt;em&gt;Summary&lt;/em&gt;, &lt;em&gt;Notes&lt;/em&gt;, &lt;em&gt;Research Method&lt;/em&gt;, and &lt;em&gt;Resources&lt;/em&gt;. The &lt;em&gt;Summary&lt;/em&gt; section will include a brief summary in my own words along with a concise summary quote from the paper itself. The &lt;em&gt;Notes&lt;/em&gt; section is pretty self explanatory but I will try to make it verbose enough that it can stand on its own. The &lt;em&gt;Research Method&lt;/em&gt; section will follow this format:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Read the introduction and summarize.&lt;/li&gt;
  &lt;li&gt;Identify the big question or hypothesis.&lt;/li&gt;
  &lt;li&gt;Summarize the background in five sentences or less.&lt;/li&gt;
  &lt;li&gt;Identify specific questions.&lt;/li&gt;
  &lt;li&gt;Identify the approach.&lt;/li&gt;
  &lt;li&gt;Read the Methods section and diagram the experiment (this will vary widely based on the paper).&lt;/li&gt;
  &lt;li&gt;Summarize the findings of each result.&lt;/li&gt;
  &lt;li&gt;Do the results answer the specific questions asked above?&lt;/li&gt;
  &lt;li&gt;Read the conclusion and summarize.&lt;/li&gt;
  &lt;li&gt;What are others saying about this paper?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the &lt;em&gt;Resources&lt;/em&gt; section will link to any additional information about the paper such as code repositories, datasets, subsequent papers, and projects based on the results of the paper.&lt;/p&gt;

&lt;h2 id=&quot;tutorials-and-other-code-based-projects&quot;&gt;Tutorials and other code based projects&lt;/h2&gt;
&lt;p&gt;My goal when working through tutorials or trying to reproduce models is to have a consistent and efficient workflow that makes it simple to replicate across projects. An efficient workflow makes it easier to understand the scope of the project and return to it at a later. At work, despite our best intentions and templated folder structure, our projects inevitably end up as a labyrinth of cryptically named folders full of unlabeled data and results. I hope that starting this project with a carefully considered organizational philosophy will help in the weeks and months to come. A few requirements that went into building my final workflow:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Always use version control.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is important because it makes it easier to work from multiple computers (and iPads), makes it easier to share and collaborate with others, and makes it easier to replicate results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Separate code from data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is especially important in machine learning projects since datasets can be very, very large. In addition, it makes it easier to swap between datasets and share code with others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Separate raw, working, and processed data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I think it‚Äôs useful to separate data into a few different sources:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Raw&lt;/strong&gt; data is the original, immutable data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interim&lt;/strong&gt; data is the working data that is being transformed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Processed&lt;/strong&gt; data is the final dataset being used for modeling.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;External&lt;/strong&gt; data is from third party sources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Organizing the data this way allows you to know when you can safely delete and move files.&lt;/p&gt;

&lt;p&gt;Given those requirements I went about building my folder structure and workflow. Quickly, though, it dawned on me that there are thousands of teams and tens of thousands of practitioners working on real, in-production problems that have most likely optimized their workflows for maximum efficiency. With that, I went in search of the perfect folder structure and research workflow. While I‚Äôm pretty sure I didn‚Äôt find exactly that, I found something that fits all the requirements above and is automated as well.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://drivendata.github.io/cookiecutter-data-science/&quot;&gt;Cookiecutter Data Science&lt;/a&gt;  project structure is &lt;em&gt;a logical, reasonably standardized, but flexible project structure for doing and sharing data science work.&lt;/em&gt; Cookiecutter Data Science is built on &lt;a href=&quot;http://cookiecutter.readthedocs.io/en/latest/readme.html&quot;&gt;Cookiecutter&lt;/a&gt; which is a command-line utility that creates projects from templates (cookiecutters). The creators of Cookiecutter Data Science summarize it like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;When we think about data analysis, we often think just about the resulting reports, insights, or visualizations. While these end products are generally the main event, it‚Äôs easy to focus on making the products &lt;em&gt;look nice&lt;/em&gt; and ignore the &lt;em&gt;quality of the code that generates them&lt;/em&gt;. Because these end products are created programmatically, code quality is still important! And we‚Äôre not talking about bikeshedding the indentation aesthetics or pedantic formatting standards ‚Äî ultimately, data science code quality is about correctness and reproducibility.&lt;/p&gt;

  &lt;p&gt;It‚Äôs no secret that good analyses are often the result of very scattershot and serendipitous explorations. Tentative experiments and rapidly testing approaches that might not work out are all part of the process for getting to the good stuff, and there is no magic bullet to turn data exploration into a simple, linear progression.&lt;/p&gt;

  &lt;p&gt;That being said, once started it is not a process that lends itself to thinking carefully about the structure of your code or project layout, so it‚Äôs best to start with a clean, logical structure and stick to it throughout. We think it‚Äôs a pretty big win all around to use a fairly standardized setup like this one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think that the template based approach is great. I have already modified the directory structure and some make files that I don‚Äôt see myself using initially. As the weeks pass and I refine my workflow, I‚Äôm sure that I will be modifying or creating new cookiecutters (you can have multiple templates that are called from the command line).&lt;/p&gt;

&lt;p&gt;I‚Äôm looking forward to my research workflow being refined over time and becoming more robust and efficient‚Äîhopefully the process described above is a good starting point.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="workflow" /><category term="yearofml" /><summary type="html">There are two categories of projects that I‚Äôve collected: ML papers and tutorials/working examples. For both of these I want a consistent workflow that allows me to present summaries in an quick and efficient manner and to reference in the future.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">A Year of Machine Learning, Neural Networks, and more</title><link href="https://www.cpdis.com/Year-of-Learning/" rel="alternate" type="text/html" title="A Year of Machine Learning, Neural Networks, and more" /><published>2018-01-01T14:38:50-06:00</published><updated>2018-01-01T14:38:50-06:00</updated><id>https://www.cpdis.com/Year-of-Learning</id><content type="html" xml:base="https://www.cpdis.com/Year-of-Learning/">&lt;p&gt;My goal this year, and with this project, is to read, replicate, and expand upon as many different papers, tutorials, and Github repositories as possible. Each week I plan on going through all the &lt;a href=&quot;https://github.com/cpdis/Experiments&quot;&gt;links&lt;/a&gt; and tabs I‚Äôve collected over the months and posting whatever the results are here. In addition, I want to improve my writing skills and the only way to do that is by practice and repetition. Please forgive my writing the first few months, it will, it has to, get better.&lt;/p&gt;

&lt;p&gt;It‚Äôs relatively easy (ü§ì) to take online &lt;a href=&quot;https://www.udacity.com/course/intro-to-machine-learning--ud120&quot;&gt;classes&lt;/a&gt;, &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;courses&lt;/a&gt;, and &lt;a href=&quot;https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013&quot;&gt;nanodegrees&lt;/a&gt; but getting your hands dirty and working through problems on your own is where real learning begins. I‚Äôm very aware that I‚Äôm probably not going to be leading authority in the field of artificial intelligence (or even someone worth a &lt;a href=&quot;https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html&quot;&gt;seven figure&lt;/a&gt; salary), that was a decision I should have made fifteen years ago. However, becoming proficient/efficient at implementing models and being up-to-date with the latest research is a huge step in the right direction.&lt;/p&gt;

&lt;p&gt;In this first week of January I‚Äôm going to try to nail down the workflow that I use to actually complete this project and to document what I learn. I have a local folder ready for any code or papers that need to be stored locally and a &lt;a href=&quot;http://www.amazon.com/dp/B075N1Z9LT/?tag=heismukamily-20&quot;&gt;NAS&lt;/a&gt; to store large datasets. Everything will be on Github for version control and to show/share my work (and so I can get more practice using Git). I‚Äôm going to be using my 2014 13‚Äù MacBook Pro for most of this project so there definitely won‚Äôt be any training time records set. In my &lt;a href=&quot;https://github.com/cpdis/P3_CarND_Behavioral_Cloning&quot;&gt;experience&lt;/a&gt;, though, simple models run quickly and long training times may incentive me to &lt;a href=&quot;https://navoshta.com/meet-fenton/&quot;&gt;upgrade my machine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The pace at which deep learning and artificial intelligence advances are being made (no matter how specialized) is truly astounding. I, for one, welcome our new ü§ñ overlords.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="machine learning" /><category term="deep learning" /><category term="artificial intelligence" /><category term="learning" /><summary type="html">My goal this year, and with this project, is to read, replicate, and expand upon as many different papers, tutorials, and Github repositories as possible. Each week I plan on going through all the links and tabs I‚Äôve collected over the months and posting whatever the results are here. In addition, I want to improve my writing skills and the only way to do that is by practice and repetition. Please forgive my writing the first few months, it will, it has to, get better.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry><entry><title type="html">The Parable of the iPhone</title><link href="https://www.cpdis.com/The-Parable-of-the-iPhone/" rel="alternate" type="text/html" title="The Parable of the iPhone" /><published>2017-12-31T14:55:01-06:00</published><updated>2017-12-31T14:55:01-06:00</updated><id>https://www.cpdis.com/The-Parable-of-the-iPhone</id><content type="html" xml:base="https://www.cpdis.com/The-Parable-of-the-iPhone/">&lt;p&gt;A 15 year old girl was picking berries from a bush one day in ancient Rome. And so it happened right before her eyes that a strange and mysterious wormhole opened up and presented an iPhone 7 Plus. She gasped in shock at the incomprehensible sight that had materialized out of thin air right before her eyes.&lt;/p&gt;

&lt;p&gt;Was it a precious metal of some kind? Perhaps a gem or a stone. It didn‚Äôt look like anything she had ever seen before. Could it be an exotic plant of some kind? A strange animal? Perhaps an extremely advanced tool. She reached her hand out to touch it. But then thought better of it. She better get her sister to take a look at it as well.&lt;/p&gt;

&lt;p&gt;She called out. Her sister was nearby and immediately ran over.&lt;/p&gt;

&lt;p&gt;‚ÄúLook at this!‚Äù, she said.&lt;/p&gt;

&lt;p&gt;Her sister‚Äôs face looked bewildered, ‚ÄúWhat is it?‚Äù&lt;/p&gt;

&lt;p&gt;‚ÄúI don‚Äôt know, it just appeared.‚Äù&lt;/p&gt;

&lt;p&gt;The sister picked up the phone. She marveled at its simplicity with affection. The vibrant silver color and precise lines of its edges. The small foreign writing on the metallic back with an image engraved in the finest handiwork of an apple that had a bite taken out.&lt;/p&gt;

&lt;p&gt;‚ÄúThis is magnificent‚Äù she said as she touched a part of the object that had a small contusion in the otherwise perfectly smooth surface. When she touched it, a brilliant light shone from the face of the object, like magic.&lt;/p&gt;

&lt;p&gt;The girls were awestruck. They thought they had better show this to their mother.&lt;/p&gt;

&lt;p&gt;A few hours later, the iPhone 7 Plus was in the hands of a Roman centurion who was taking a slow-mo video of his own face when the object disappeared as suddenly as it had arrived.&lt;/p&gt;

&lt;p&gt;A similar thing happened in two other places at two other times. The wormhole had opened. The iPhone had appeared. Once in South America to a young man in a hunter gatherer society and once to a middle aged woman in Manila in 1968. The result of these three temporary appearances of the iPhone 7 Plus shook the cultures that the iPhone had presented itself to to the core. Each had named the events and the object in different ways.&lt;/p&gt;

&lt;p&gt;The Romans had interpreted the event as an act of Caesar to display his power. The phone ended up being named the Delustricus. They started a holiday to celebrate the gracious revelation their Lord had bestowed upon them.&lt;/p&gt;

&lt;p&gt;The South American hunter gatherers had named the object what can be translated roughly as Sun Jewel, they ended up worshipping the Sun Jewel as their prime deity. Sacrifice by fire was the deity‚Äôs choice of preferred worship.&lt;/p&gt;

&lt;p&gt;The Filipino‚Äôs that had encountered the phone had eventually come to the conclusion that the device was an alien machine of infinite power that they called the Nuckacot. It happened to appear on a Tuesday precisely while the woman who had discovered it was overcooking a pot of rice. The culture had developed a custom of overcooking rice on Tuesdays in order to appease the aliens and pray for the return of the mysterious and beautiful Nuckacot.&lt;/p&gt;

&lt;p&gt;You see, this is what human beings do when we don‚Äôt have words for something, we make up words for them. Even if the thing we are trying to talk about is not a thing, is beyond thingness, but an experience that transcends all of our language and conceptions. We still try to find words, we still try to make meaning and tell stories. And so we get religion with all of its different variations and forms and practices.&lt;/p&gt;

&lt;p&gt;Are all religions the same? No, of course not.&lt;/p&gt;

&lt;p&gt;Neither are all the explanations of the iPhone. They are actually quite different.&lt;/p&gt;

&lt;p&gt;But is the iPhone the same? Of course it is.&lt;/p&gt;</content><author><name>cpdis</name></author><category term="blog" /><category term="apple" /><category term="technology" /><category term="iphone" /><category term="religion" /><summary type="html">A 15 year old girl was picking berries from a bush one day in ancient Rome. And so it happened right before her eyes that a strange and mysterious wormhole opened up and presented an iPhone 7 Plus. She gasped in shock at the incomprehensible sight that had materialized out of thin air right before her eyes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cpdis.com/assets/images/" /></entry></feed>